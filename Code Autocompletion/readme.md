# Code Autocompletion
Project Inspired by **Prof.Chaky** and fully support from our TA **Amanda**.
- Prof.github: https://github.com/chaklam-silpasuwanchai/Python-for-NLP
## Code Parrot-GPT-2
Learn more: [https://nlp.stanford.edu/sentiment/treebank.html](https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text)

### Suggestion result
<img src="https://github.com/rambosorn/NLP_Project/blob/main/Code%20Autocompletion/image/codeparrot.png" alt="Alt text" title="Optional title">

_I have tried with code parrot by hugging face build on top of GPT-2. The Code Parrot dataset is already preprocessed and split into training and validation sets, and the GPT-2 language model is pre-trained on a large corpus of text data. However the generat suggestion result seem not accurate. It might due for some reason: This could be due to a variety of reasons, such as the model not being trained on enough data, or the input code being too complex for the model to accurately predict the next few lines._

## Use OpenAI's GPT-3 API
1. Sign up for an API key on the OpenAI website: https://beta.openai.com/signup/
<img src="https://github.com/rambosorn/NLP_Project/blob/main/Code%20Autocompletion/image/image_2023-02-26_02-45-22.png" alt="Alt text" title="Optional title">

_The choice between Code Parrot and GPT-3 ultimately depends on the specific needs and use case of the project. For tasks that require high accuracy in code autocompletion, Code Parrot may be the better choice, while GPT-3 may be more suitable for tasks that require a more general-purpose language model._

