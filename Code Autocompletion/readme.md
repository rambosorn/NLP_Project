# Code Autocompletion
Project Inspired by **Prof.Chaky** and fully support from our TA **Amanda**.
- Prof.github: https://github.com/chaklam-silpasuwanchai/Python-for-NLP
### Code Parrot-GPT-2
Learn more: [https://nlp.stanford.edu/sentiment/treebank.html](https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text)

## Suggestion result
<img src="https://github.com/rambosorn/NLP_Project/blob/main/Code%20Autocompletion/image/codeparrot.png" alt="Alt text" title="Optional title">

__I have tried with code parrot by hugging face build on top of GPT-2. The Code Parrot dataset is already preprocessed and split into training and validation sets, and the GPT-2 language model is pre-trained on a large corpus of text data. However the generat suggestion result seem not accurate. It might due for some reason: This could be due to a variety of reasons, such as the model not being trained on enough data, or the input code being too complex for the model to accurately predict the next few lines.

